/*
* Copyright (c) 2020-present SMC Treviso s.r.l. All rights reserved.
*
* This program is free software: you can redistribute it and/or modify
* it under the terms of the GNU Affero General Public License as published by
* the Free Software Foundation, either version 3 of the License, or
* (at your option) any later version.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
* GNU Affero General Public License for more details.
*
* You should have received a copy of the GNU Affero General Public License
* along with this program.  If not, see <http://www.gnu.org/licenses/>.
*/
import { gql } from "@apollo/client";
import { TemplateType } from "@pages/Analyzer/gql";

gql`
  query Tokenizer($id: ID!) {
    tokenizer(id: $id) {
      id
      name
      description
      jsonConfig
      type
    }
  }
`;

gql`
  mutation CreateOrUpdateTokenizer($id: ID, $name: String!, $description: String, $jsonConfig: String, $type: String!) {
    tokenizer(id: $id, tokenizerDTO: { name: $name, description: $description, jsonConfig: $jsonConfig, type: $type }) {
      entity {
        id
        name
      }
      fieldValidators {
        field
        message
      }
    }
  }
`;

gql`
  query Tokenizers($searchText: String, $after: String) {
    tokenizers(searchText: $searchText, first: 20, after: $after) {
      edges {
        node {
          id
          name
          description
        }
      }
      pageInfo {
        hasNextPage
        endCursor
      }
    }
  }
`;

gql`
  mutation DeleteTokenizer($id: ID!) {
    deleteTokenizer(tokenizerId: $id) {
      id
      name
    }
  }
`;

export const TemplateTokenizer: TemplateType[] = [
  {
    title: "thai",
    description:
      "The thai tokenizer segments Thai text into words, using the Thai segmentation algorithm included with Java. Text in other languages in general will be treated the same as the standard tokenizer.",
    type: "thai",
    value: [],
  },
  {
    title: "classic",
    description:
      "The classic tokenizer is a grammar based tokenizer that is good for English language documents. This tokenizer has heuristics for special treatment of acronyms, company names, email addresses, and internet host names.",
    type: "classic",
    value: [],
  },
  {
    title: "edge_ngram",
    description: "Performs optional post-processing of terms generated by the classic tokenizer.",
    type: "edge_ngram",
    value: [
      {
        name: "min_gram",
        value: 1,
        type: "number",
        description: "Minimum length of characters in a gram. Defaults to 1.",
      },
      {
        name: "max_gram",
        value: 2,
        type: "number",
        description: "Maximum length of characters in a gram. Defaults to 2.",
      },
      {
        name: "token_chars",
        value: ["letter"],
        type: "multi-select",
        description:
          "Character classes that should be included in a token. Elasticsearch will split on characters that donâ€™t belong to the classes specified. Defaults to [] (keep all characters).",
      },
    ],
  },
  {
    title: "keyword",
    description:
      "The keyword tokenizer is a â€œnoopâ€ tokenizer that accepts whatever text it is given and outputs the exact same text as a single term. It can be combined with token filters to normalise output, e.g. lower-casing email addresses.",
    type: "keyword",
    value: [],
  },
  {
    title: "letter",
    description:
      "The letter tokenizer breaks text into terms whenever it encounters a character which is not a letter. It does a reasonable job for most European languages, but does a terrible job for some Asian languages, where words are not separated by spaces.",
    type: "letter",
    value: [],
  },
  {
    title: "lowercase",
    description:
      "The lowercase tokenizer, like the letter tokenizer breaks text into terms whenever it encounters a character which is not a letter, but it also lowercases all terms. It is functionally equivalent to the letter tokenizer combined with the lowercase token filter, but is more efficient as it performs both steps in a single pass.",
    type: "lowercase",
    value: [],
  },
  {
    title: "ngram",
    description:
      "The ngram tokenizer first breaks text down into words whenever it encounters one of a list of specified characters, then it emits N-grams of each word of the specified length.",
    type: "ngram",
    value: [
      {
        name: "min_gram",
        value: 1,
        type: "number",
        description: "Minimum length of characters in a gram. Defaults to 1",
      },
      {
        name: "max_gram",
        value: 2,
        type: "number",
        description: "Maximum length of characters in a gram. Defaults to 2.",
      },
      {
        name: "token_chars",
        value: ["letter"],
        type: "multi-select",
        description:
          "Character classes that should be included in a token. Elasticsearch will split on characters that donâ€™t belong to the classes specified. Defaults to [] (keep all characters).",
      },
      {
        name: "custom_token_chars",
        value: "+-_",
        type: "string",
        description:
          "Custom characters that should be treated as part of a token. For example, setting this to +-_ will make the tokenizer treat the plus, minus and underscore sign as part of a token.",
      },
    ],
  },
  {
    title: "path_hierarchy",
    description:
      "The path_hierarchy tokenizer takes a hierarchical value like a filesystem path, splits on the path separator, and emits a term for each component in the tree.",
    type: "path_hierarchy",
    value: [
      {
        name: "delimiter",
        value: "/",
        type: "string",
        description: "The character to use as the path separator. Defaults to /.",
      },
      {
        name: "replacement",
        value: "-",
        type: "string",
        description: "An optional replacement character to use for the delimiter. Defaults to the delimiter.",
      },
      { name: "skip", value: 0, type: "number", description: "The number of initial tokens to skip. Defaults to 0." },
      {
        name: "buffer_size",
        value: 1024,
        type: "number",
        description:
          "The number of characters read into the term buffer in a single pass. Defaults to 1024. The term buffer will grow by this size until all the text has been consumed. It is advisable not to change this setting.",
      },
      {
        name: "reverse",
        value: true,
        type: "boolean",
        description: "If true, generates tokens in reverse order. Default is false.",
      },
    ],
  },
  {
    title: "pattern",
    description:
      "The pattern tokenizer uses a regular expression to either split text into terms whenever it matches a word separator, or to capture matching text as terms.",
    type: "pattern",
    value: [
      {
        name: "pattern",
        value: "\\W+",
        type: "string",
        description: "Java regular expression flags. Flags should be pipe-separated, ex: 'CASE_INSENSITIVE|COMMENTS'.",
      },
      { name: "flags", value: "", type: "string", description: "A Java regular expression, defaults to W+." },
      { name: "group", value: 1, type: "number", description: "1" },
    ],
  },
  {
    title: "simple_pattern",
    description:
      "The simple_pattern tokenizer uses a regular expression to capture matching text as terms. The set of regular expression features it supports is more limited than the pattern tokenizer, but the tokenization is generally faster.",
    type: "simple_pattern",
    value: [
      {
        name: "pattern",
        value: "",
        type: "string",
        description: "Lucene regular expression, defaults to the empty string.",
      },
    ],
  },
  {
    title: "simple_pattern_split",
    description:
      "The simple_pattern_split tokenizer uses a regular expression to split the input into terms at pattern matches. The set of regular expression features it supports is more limited than the pattern tokenizer, but the tokenization is generally faster.",
    type: "simple_pattern_split",
    value: [
      {
        name: "pattern",
        value: "",
        type: "string",
        description: "Lucene regular expression, defaults to the empty string.",
      },
    ],
  },
  {
    title: "standard",
    description:
      "The standard tokenizer provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29) and works well for most languages.",
    type: "standard",
    value: [
      {
        name: "max_token_length",
        value: 255,
        type: "number",
        description:
          "The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255.",
      },
    ],
  },
  {
    title: "uax_url_email",
    description:
      "The uax_url_email tokenizer is like the standard tokenizer except that it recognises URLs and email addresses as single tokens.",
    type: "uax_url_email",
    value: [
      {
        name: "max_token_length",
        value: 255,
        type: "number",
        description:
          "The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255.",
      },
    ],
  },
  {
    title: "whitespace",
    description: "The whitespace tokenizer breaks text into terms whenever it encounters a whitespace character.",
    type: "whitespace",
    value: [
      {
        name: "max_token_length",
        value: 255,
        type: "number",
        description:
          "The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255.",
      },
    ],
  },
];

